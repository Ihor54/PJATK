Faits marquants depuis les années 2000[modifier | modifier le code]
L'intelligence artificielle est un sujet d'actualité au xxie siècle. En 2004, l'Institut Singularity a lancé une campagne Internet appelée « Trois lois dangereuses » : « Three Laws Unsafe » (en lien avec les trois lois d'Asimov) pour sensibiliser aux questions de la problématique de l'intelligence artificielle et l'insuffisance des lois d'Asimov en particulier. (Singularity Institute for Artificial Intelligence 2004)14.
En 2005, le projet Blue Brain est lancé, il vise à simuler le cerveau des mammifères. Il s'agit d'une des méthodes envisagées pour réaliser une IA. Ils annoncent de plus comme objectif de fabriquer, dans dix ans, le premier « vrai » cerveau électronique15. En mars 2007, le gouvernement sud-coréen a annoncé que plus tard dans l'année, il émettrait une charte sur l'éthique des robots, afin de fixer des normes pour les utilisateurs et les fabricants. Selon Park Hye-Young, du ministère de l'Information et de la communication, la Charte reflète les trois lois d'Asimov : la tentative de définition des règles de base pour le développement futur de la robotique. En juillet 2009, Californie, conférence organisé par l'Association for the Advancement of Artificial Intelligence (AAAI), où un groupe d'informaticiens se demande s'il devrait y avoir des limites sur la recherche qui pourrait conduire à la perte de l'emprise humaine sur les systèmes informatiques, et où il était également question de l'explosion de l'intelligence (artificielle) et du danger de la singularité technologique conduisant à un changement d'ère, ou de paradigme totalement en dehors du contrôle humain16,17.
En 2009, le Massachusetts Institute of Technology (MIT) a lancé un projet visant à repenser la recherche en intelligence artificielle. Il réunira des scientifiques qui ont eu du succès dans des domaines distincts de l'IA. Neil Gershenfeld déclare « Nous voulons essentiellement revenir 30 ans en arrière, et de revoir quelques directions aujourd'hui gelées »18.
En novembre 2009, l'US Air Force cherche à acquérir 2 200 PlayStation 319 pour utiliser le processeur cell à 7 ou 8 cœurs qu'elle contient dans le but d'augmenter les capacités de leur superordinateur constitué de 336 PlayStation 3 (total théorique 52,8 PetaFlops en double précision). Le nombre sera réduit à 1 700 unités le 22 décembre 200920. Le projet vise le traitement vidéo haute-définition, et l'« informatique neuromorphique », ou la création de calculateurs avec des propriétés/fonctions similaires au cerveau humain19.
Le 27 janvier 2010, l'US Air Force demande l'aide de l'industrie pour développer une intelligence avancée de collecte d'information et avec la capacité de décision rapide pour aider les forces américaines pour attaquer ses ennemis rapidement à leurs points les plus vulnérables. L'US Air Force utilisera une intelligence artificielle, le raisonnement ontologique, et les procédures informatique basées sur la connaissance, ainsi que d'autres traitement de données avancées afin de frapper l'ennemi au meilleur point21. D'autre part, d’ici 2020, plus de mille bombardiers et chasseurs F-22 et F-35 de dernière génération, parmi plus de 2 500 avions militaires, commenceront à être équipés de sorte que, d’ici 2040, tous les avions de guerre américains soient pilotés par intelligence artificielle, en plus des 10 000 véhicules terrestres et des 7 000 dispositifs aériens commandés d'ores et déjà à distance22.
Le 16 février 2011, Watson, le superordinateur conçu par IBM, remporte deux des trois manches du jeu télévisé Jeopardy! en battant largement ses deux concurrents humains en gains cumulés. Pour cette IA, la performance a résidé dans le fait de répondre à des questions de culture générale (et non un domaine technique précis) dans des délais très courts. En février 2016, l'artiste et designer Aaron Siegel propose de faire de Watson un candidat à l'élection présidentielle américaine afin de lancer le débat sur « le potentiel de l’intelligence artificielle dans la politique »23.
En mai 2013, Google ouvre un laboratoire de recherches dans les locaux de la NASA. Grâce à un super ordinateur quantique conçu par D-Wave Systems et qui serait d'après cette société 11 000 fois plus performant qu'un ordinateur actuel (de 2013)24, ils espèrent ainsi faire progresser l'intelligence artificielle et notamment l'apprentissage automatique. Raymond Kurzweil est engagé en décembre 2012 par Google afin de participer et d'améliorer l'apprentissage automatique des machines et des IA25.
En 2014-2015, à la suite du développement rapide du deep learning, et à l'encontre des penseurs transhumanistes, quelques scientifiques et membres de la communauté high tech craignent que l'intelligence artificielle ne vienne à terme dépasser les performances de l'intelligence humaine. Parmi eux, l'astrophysicien britannique Stephen Hawking26, le fondateur de Microsoft Bill Gates27 et le PDG de Tesla Elon Musk28.
Les géants de l'Internet s'intéressent de plus en plus à l'IA29. Le 3 janvier 2016, le patron de Facebook, Mark Zuckerberg, s’est donné pour objectif de l’année de « construire une intelligence artificielle simple pour piloter ma maison ou m’aider dans mon travail »[réf. nécessaire]. Il avait déjà créé en 2013 le laboratoire Facebook Artifical Intelligence Research (FAIR) dirigé par le chercheur français Yann Le Cun et ouvert un laboratoire de recherche permanente dans le domaine à Paris30.
Apple a de son côté récemment acquis plusieurs start-up du secteur (Perceptio, VocalIQ, Emotient et Turi)31.
En janvier 2018, des modèles d'intelligence artificielle développés par Microsoft et Alibaba réussissent chacun de leur côté à battre les humains dans un test de lecture et de compréhension de l'Université de Stanford. Le traitement du langage naturel imite la compréhension humaine des mots et des phrases et permet maintenant aux modèles d'apprentissage automatique de traiter de grandes quantités d'informations avant de fournir des réponses précises aux questions qui leur sont posées32.
En France[modifier | modifier le code]
En France, les pionniers sont Alain Colmerauer, Gérard Huet, Jean-Louis Laurière, Claude-François Picard, Jacques Pitrat et Jean-Claude Simon33. Un congrès national annuel Reconnaissance de formes et intelligence artificielle est créé en 1979 à Toulouse34. En lien avec l'organisation de la conférence IJCAI (en) à Chambéry en 1993, et la création d'un GRECO-PRC35 intelligence artificielle, en 1983, il donne naissance à une société savante, l'AFIA en 1989, qui, entre autres, organise des conférences nationales en intelligence artificielle36.

Logo de la conférence AI for Humanity organisée le 29 mars 2018 au Collège de France.
Au début des années 2000, des sociologues liés à l'EHESS, spécialistes de l'analyse de corpus, expérimentent la mise au point d'un « sociologue numérique » appelé Marlowe qui crée un pont entre intelligence artificielle et sciences sociales37. Dès le début de l'année 2017, les initiatives se multiplient et s'accélèrent[pourquoi ?]. Le 17 janvier 2017, le fonds de capital-risque Serena Capital lance un fonds de 80 millions d’euros destiné à l’investissement dans les start-ups européennes du big data et de l'intelligence artificielle38. Le 19 janvier 2017, une audition se tient au Sénat « L'intelligence Artificielle menace-t-elle nos emplois ? »39. Le 20 janvier 2017, Axelle Lemaire entend valoriser les potentiels scientifiques et industriels français grâce au projet « France IA »40. En janvier 2017, dans le cadre de sa mission de réflexion sur les enjeux éthiques et les questions de société soulevés par les technologies numériques, la CNIL annonce l'organisation d'un débat public sur les algorithmes et l'intelligence artificielle41. Le 15 décembre 2017, à l'issue d'un débat ayant mobilisé 60 partenaires (institutions publiques, associations, entreprises, acteurs du monde de la recherche, société civile)42, elle publie son rapport "Comment permettre à l'Homme de garder la main ?"43 comprenant des recommandations pour la construction d'un modèle éthique d'intelligence artificielle. En septembre 2017, Cédric Villani, premier vice-président de l'OPECST44, est chargé de mener une consultation publique sur l'intelligence artificielle45. Il rend son rapport le 28 mars 201846, à la veille d'une intervention du président de la République Française Emmanuel Macron au Collège de France pour annoncer la stratégie de la France dans ce domaine47. Il y dévoile un plan de 1,5 milliards d'euros sur l'ensemble du quinquennat, ainsi qu'une évolution de la législation française pour permettre la mise en application de l'intelligence artificielle, en particulier concernant la circulation des véhicules autonomes48. Parallèlement à ces annonces, il est interviewé par Wired, magazine de référence pour la communauté mondiale des nouvelles technologies et y exprime sa vision de l'intelligence artificielle, à savoir que les algorithmes utilisés par l'État doivent être ouverts, que l'intelligence artificielle doit être encadrée par des règles philosophiques et éthiques et qu'il faut s'opposer à l'usage d'armes automatiques ou de dispositifs prenant des décisions sans consulter un humain49,50.
Test de Turing[modifier | modifier le code]

Schéma du test de Turing.
Article détaillé : Test de Turing.
À l’orée des années 1950, entre la naissance de la cybernétique et l’émergence quelques années plus tard de l’intelligence artificielle, alors que les meilleurs esprits du temps s’interrogent sur la possibilité de construire des machines pensantes, Alan Turing propose, dès le début d’un article demeuré célèbre, un test pour déterminer si une machine peut être définie comme « consciente »51.
Définir l’intelligence est un défi et il n’est pas certain qu’on puisse y arriver un jour d’une façon satisfaisante. C’est cette remarque qui poussa le mathématicien britannique Alan Turing, il y a soixante ans, à proposer « le jeu de l’imitation » qui fixait un objectif précis à la science naissante des ordinateurs que l'on n'appelait pas encore informatique en francophonie. Ce « jeu de l'imitation » suggérait qu'un juge fictif puisse dialoguer d'une part avec une machine et d'autre part avec un humain à l'aide d'un terminal sans pouvoir les discriminer52.
Jusqu'à ce jour, aucun logiciel n'a encore pu réussir ce test, à savoir se comporter de façon à ne pas être discriminé d'un humain, malgré de nombreuses tentatives. Devant la persistance de ces échecs certains informaticiens[Lesquels ?] pensent que mettre au point un programme aussi complexe ne démontrera pas l'intelligence des programmes ni leur capacité à penser.
De nos jours, une machine peut certes réviser et faire évoluer des objectifs qu’on lui a attribués. Une machine peut même être programmée pour pouvoir restructurer sa connaissance initiale à partir d’informations reçues ou perçues. Mais la machine d’aujourd’hui ne pense pas à proprement parler, car elle n’a pas conscience d’elle-même (et en particulier de ses limites), elle ne peut pas ultimement décider de ses buts ni imaginer de nouvelles formes de représentations du monde51.
Intelligence artificielle forte[modifier | modifier le code]
Article détaillé : Philosophie de l'intelligence artificielle.
Définition[modifier | modifier le code]
Le concept d’intelligence artificielle forte fait référence à une machine capable non seulement de produire un comportement intelligent, mais d’éprouver une impression d'une réelle conscience de soi, de « vrais sentiments » (quoi qu’on puisse mettre derrière ces mots), et « une compréhension de ses propres raisonnements »53.
L’intelligence artificielle forte a servi de moteur à la discipline, mais a également suscité de nombreux débats. En se fondant sur l'hypothèse, que tendent à confirmer les neurosciences et que des chercheurs n'hésitent pas à affirmer54, que la conscience a un support biologique et donc matériel, les scientifiques ne voient généralement pas d’obstacle de principe à créer un jour une intelligence consciente sur un support matériel autre que biologique. Selon les tenants de l'IA forte, si à l'heure actuelle il n'y a pas d'ordinateurs ou de robots aussi intelligents que l'être humain, ce n'est pas un problème d'outil mais de conception. Il n'y aurait aucune limite fonctionnelle (un ordinateur est une machine de Turing universelle avec pour seules limites les limites de la calculabilité), il n'y aurait que des limites liées à l'aptitude humaine à concevoir les logiciels appropriés (programme, base de données...). Elle permet notamment de modéliser des idées abstraites.
Estimation de faisabilité[modifier | modifier le code]
Comparer la capacité de traitement de l'information d'un cerveau humain à celle d'un ordinateur peut aider à comprendre les ordres de grandeur pour estimer la possibilité pratique ou non d'une intelligence artificielle forte, de même qu'un simple calcul de puissance en kW permet grosso modo de dire qu'un camion donné pourra espérer transporter commodément telle ou telle charge ou si cela lui sera impossible. Voici quelques exemples d'ordres de grandeur en traitement de l'information :
Balance Roberval : 1 bit par seconde (comparaison de deux poids) ;
mainframe typique des années 1970 : 1 million d'opérations par seconde sur 32 bits ;
Intel Paragon XP/S, 4 000 processeurs i860 @ 50 MHz (1992) : 160 milliards d'opérations par seconde.
Cette puissance n'est pas à prendre au pied de la lettre. Elle précise surtout les ordres de grandeur en présence et leur évolution relativement rapide (2015).
L'intelligence artificielle n'avait donné que des résultats mitigés sur les ordinateurs typiques de 1970 effectuant 107 opérations logiques par seconde55,56. Le cerveau humain, formé de 1011 neurones ne pouvant chacun commuter plus de 100 fois par seconde en raison de leur temps de relaxation permettait beaucoup plus de traitements logiques par unité de temps (1013 opérations logiques par seconde)56. Ce handicap technique précis n'existe plus sur les ordinateurs actuels (2015), travaillant en 64 bits et avec des horloges cadencées à 4 GHz environ.
Il s'agit là de processeurs destinés au particulier. Concernant des machines comme Blue Gene, le rapport du nombre de comparaisons par seconde entre ordinateur et cerveau ont même changé de sens.
Un article de 201357 examine par plusieurs voies quelle pourrait être la capacité mémoire nécessaire et, selon le mode de calcul, obtient des chiffres très différents : 1 To, 100 To, 2 500 To (voir big data), évoquant aussi par jeu 300 Mo, soit 60 MP3 de 3 minutes.
Le matériel serait donc maintenant présent. Du logiciel à la mesure de ce matériel resterait à développer. En effet, l'important n'est pas de raisonner plus vite, en traitant plus de données, ou en mémorisant plus de choses que le cerveau humain58, l'important est de traiter les informations de manière appropriée.
L'IA souligne la difficulté à expliciter toutes les connaissances utiles à la résolution d'un problème complexe. Certaines connaissances dites implicites sont acquises par l'expérience et mal formalisables. Par exemple, qu'est-ce qui distingue un visage familier de deux cents autres ? Nous ne savons pas toujours clairement l'exprimer59. L'apprentissage de ces connaissances implicites par l'expérience est exploitée depuis les années 1980 (voir Réseau de neurones). Néanmoins, un autre type de complexité apparaît : la complexité structurelle. Comment mettre en relation des modules spécialisés pour traiter un certain type d'informations, par exemple un système de reconnaissance des formes visuelles, un système de reconnaissance de la parole, un système lié à la motivation, à la coordination motrice, au langage, etc. En revanche, une fois un système cognitif conçu et son apprentissage par l'expérience réalisé, l'« intelligence » correspondante peut être distribuée en un grand nombre d'exemplaires, par exemple sur les portables d'actuaires ou de banquiers pouvant ainsi, comme le rappelle un slogan, dire oui ou non, mais le dire tout de suite grâce à des applications dites de credit scoring.
Enfin, au-delà de la simple technique, il reste à savoir de quel type serait la relation entre l'homme et la machine intelligente : bonne et peut-être non exempte de bugs, comme dans Les Robots d'Asimov, ou carrément hostile comme dans Terminator sont deux exemples possibles60.
Diversité des opinions[modifier | modifier le code]
Les principales opinions soutenues pour répondre à la question d’une intelligence artificielle consciente sont les suivantes :
Impossible : la conscience serait le propre des organismes vivants, et elle serait liée à la nature des systèmes biologiques. Cette position est défendue principalement par des religieux.
Problème : Elle rappelle toutefois toutes les controverses passées entre vitalistes et matérialistes.
Impossible avec des machines manipulant des symboles comme les ordinateurs actuels, mais possible avec des systèmes dont l’organisation matérielle serait fondée sur des processus quantiques. Cette position est défendue notamment par Roger Penrose. Des algorithmes quantiques sont théoriquement capables de mener à bien des calculs hors de l'atteinte pratique des calculateurs conventionnels (complexité en {\displaystyle N\ln(N)~} N\ln(N)~ au lieu de {\displaystyle N^{2}~} N^{2}~, par exemple, sous réserve d'existence du calculateur approprié). Au-delà de la rapidité, le fait que l'on puisse envisager des systèmes quantiques en mesure de calculer des fonctions non-turing-calculables (voir Hypercalcul) ouvre des possibilités qui - selon cet auteur - sont fondamentalement interdites aux machines de Turing.
Problème : On ne dispose pas encore pour le moment d'algorithmes d'IA à mettre en œuvre dans ce domaine. Tout cela reste donc spéculatif.
Impossible avec des machines manipulant des symboles comme les ordinateurs actuels, mais possible avec des systèmes dont l’organisation matérielle mimerait le fonctionnement du cerveau humain, par exemple avec des circuits électroniques spécialisés reproduisant le fonctionnement des neurones.
Problème : Le système en question répondant exactement de la même façon que sa simulation sur ordinateur - toujours possible - au nom de quel principe leur assigner une différence ?61
Impossible avec les algorithmes classiques manipulant des symboles (logique formelle), car de nombreuses connaissances sont difficiles à expliciter mais possible avec un apprentissage par l'expérience de ces connaissances à l'aide d'outils tels que des réseaux de neurones formels, dont l'organisation logique et non matérielle s'inspire des neurones biologiques, et utilisés avec du matériel informatique conventionnel.
Problème : si du matériel informatique conventionnel est utilisé pour réaliser un réseau de neurones, alors il est possible de réaliser l'IA avec les ordinateurs classiques manipulant des symboles (puisque ce sont les mêmes machines, voir Thèse de Church-Turing). Cette position parait donc incohérente. Toutefois, ses défenseurs (thèse de l'IA forte) arguent que l'impossibilité en question est liée à notre inaptitude à tout programmer de manière explicite, elle n'a rien à voir avec une impossibilité théorique. Par ailleurs, ce que fait un ordinateur, un système à base d'échanges de bouts de papier dans une salle immense peut le simuler quelques milliards de fois plus lentement. Or il peut rester difficile à admettre que cet échange de bouts de papiers « ait une conscience ». Voir Chambre chinoise. Selon les tenants de l'IA forte, cela ne pose toutefois pas de problème.
Impossible car la pensée n'est pas un phénomène calculable par des processus discrets et finis. Pour passer d'un état de pensée au suivant, il y a une infinité non dénombrable, une continuité d'états transitoires. Cette idée est réfutée par Alain Cardon (Modéliser et concevoir une Machine pensante).
Possible avec des ordinateurs manipulant des symboles. La notion de symbole est toutefois à prendre au sens large. Cette option inclut les travaux sur le raisonnement ou l'apprentissage symbolique basé sur la logique des prédicats, mais aussi les techniques connexionnistes telles que les réseaux de neurones, qui, à la base, sont définies par des symboles. Cette dernière opinion constitue la position la plus engagée en faveur de l'intelligence artificielle forte.
Des auteurs comme Douglas Hofstadter (mais déjà avant lui Arthur C. Clarke ou Alan Turing) (voir le test de Turing) expriment par ailleurs un doute sur la possibilité de faire la différence entre une intelligence artificielle qui éprouverait réellement une conscience, et une autre qui simulerait exactement ce comportement. Après tout, nous ne pouvons même pas être certains que d’autres consciences que la nôtre, y compris chez des humains, éprouvent réellement quoi que ce soit, si ce n'est par une pétition de principe qui spécule que chaque humain se retrouve à l'identique chez tous les autres. On retrouve là le problème connu du solipsisme en philosophie.
Travaux complémentaires[modifier | modifier le code]
Le mathématicien de la physique Roger Penrose62 pense que la conscience viendrait de l'exploitation de phénomènes quantiques dans le cerveau (voir microtubules), empêchant la simulation réaliste de plus de quelques dizaines de neurones sur un ordinateur normal, d’où les résultats encore très partiels de l’IA. Il restait jusqu’à présent isolé sur cette question. Un autre chercheur a présenté depuis une thèse de même esprit quoique moins radicale : Andrei Kirilyuk63.
Cette spéculation reste néanmoins marginale par rapport aux travaux des neurosciences. L'action de phénomènes quantiques est évidente[réf. nécessaire] dans le cas de la rétine (quelques quanta de lumière seulement suffisent à une perception) ou de l'odorat, mais elle ne constitue pas une condition préalable à un traitement efficace de l'information. En effet, le traitement de l'information effectué par le cerveau est relativement robuste[réf. nécessaire] et ne dépend pas de l'état quantique de chaque molécule, ni même de la présence ou de la connexion de neurones isolés.
Cela dit, l’intelligence artificielle est loin de se limiter aux seuls réseaux de neurones, qui ne sont généralement utilisés que comme classifieurs. Les techniques de résolution générale de problèmes et la logique des prédicats64, entre autres, ont fourni des résultats significatifs et sont exploités par des ingénieurs et chercheurs dans plusieurs domaines (en particulier depuis Mycin (en) en 1973 pour le diagnostic des maladies du sang).