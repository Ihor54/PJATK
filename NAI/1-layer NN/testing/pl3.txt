Prefiks WWW[edytuj | edytuj kod]
Wiele adresów internetowych zaczyna sie od „www” ze wzgledu na dlugoletnia praktyke nazywania hostów internetowych (serwerów) zgodnie z uslugami, które oferowaly. Nazwa hosta dla serwera Web to najczesciej www, tak jak ftp dla serwera FTP czy news lub nntp dla serwerów informacyjnych Usenet. Te nazwy hostów ukazuja sie, jako subdomeny w Domain Name System (DNS), jak w przykladzie www.example.com. Stosowanie takich subdomen nie jest wymagane. Pierwszy na swiecie serwer Web nazywal sie nxoc01.cern.ch, a wiele stron internetowych istnieje bez prefiksu WWW, czy innych takich jak „www2”, „secure” itp. Prefiksy subdomen nie maja zadnego praktycznego znaczenia, sa to zwykle nazwy nadane przez administratorów. Wiele serwerów internetowych jest tak skonfigurowanych, aby korzystac z obu wersji adresu, zarówno samej domeny (example.com), jak i z subdomena (www.example.com). W praktyce kieruja one uzytkownika dokladnie do tej samej strony.
W przypadku wpisania tylko jednego specyficznego slowa w pasku adresu przegladarki, np.: apple <enter>, openoffice <enter> aplikacja moze sama spróbowac dodac przedrostek www i koncówke np.: „.com”, „.org” lub „.net” i przekieruje nas np. na strone „http://www.apple.com/”, czy http://www.openoffice.org/. Funkcje te zostaly wprowadzone we wczesnych wersjach przegladarki Mozilla Firefox na poczatku roku 2003. Natomiast Microsoft otrzymal w 2008 patent na to samo rozwiazanie, z tym, ze tylko w odniesieniu do urzadzen mobilnych.
Przedrostki „http://” i „https://” nalezy rozrózniac. Hypertext Transfer Protocol (HTTP) i HTTP Secure wyznaczaja protokól komunikacyjny, który ma zostac uzyty do wysylania i pobierania zawartosci strony. Protokól HTTP jest podstawowym elementem dzialania struktury www, a HTTPS dodaje niezbedna warstwe ochronna w przypadku, gdy poufne informacje, takie jak hasla czy dane bankowe, maja byc przesylane w publicznej sieci Internet. Przegladarki internetowe równiez automatycznie dopisuja ten element (HTTPS), jesli zostanie on pominiety. Ogólny zarys RFC 2396 ? okreslajacy postac adresów internetowych to: <protokól>://<host><sciezka>?<zapytanie>#<fragment>, gdzie <host> to np. serwer internetowy (jak www.example.com), a sciezka identyfikuje konkretna podstrone. Serwer przetwarza <zapytanie>, które moze np. za posrednictwem formularza wyslac dane do zewnetrznej wyszukiwarki, poprzez to zawartosc wyswietlanej strony jest zalezna od odebranych informacji zwrotnych. <fragment> nie jest wysylany do serwera. Okresla, która czesc strony ma byc wyswietlana uzytkownikowi domyslnie.
W jezyku angielskim www wymawiane jest przez pojedyncze wypowiedzenie ciagu znaków (double-u double-u double-u). Niektóre kregi uzytkowników wymawiaja dub-dub-dub, aczkolwiek ten sposób nie jest jeszcze zbyt powszechny. Angielski pisarz Douglas Adams zazartowal kiedys w „The Independent on Sunday” (1999): „World Wide Web jest z tego, co wiem, jedynym wyrazeniem, którego skrócona forma jest trzy razy dluzsza od pelnej”. Okreslenie World Wide Web jest powszechnie tlumaczone na jezyk chinski jako: wan wei wang, co doslownie oznacza „mnóstwo wymiarów sieci”. Tlumaczenie to bardzo dobrze odzwierciedla koncepcje projektu i zalozenia WWW. Tim Berners-Lee zdefiniowal, iz wyrazenie World Wide Web powinno byc pisane jako 3 osobne slowa bez zadnych dodatkowych laczników.
Prywatnosc[edytuj | edytuj kod]
Uzytkownicy komputerów, którzy oszczedzaja czas i pieniadze, a takze ci, którzy poszukuja wygody i rozrywki, sa narazeni na utrate prywatnosci w sieci. Na calym swiecie ponad pól miliarda osób korzysta z serwisów spolecznosciowych, a mlodziez dorastajaca w dobie Internetu dokonuje kolejnej zmiany pokoleniowej. Z Facebooka, poczatkowo rozpowszechnionego posród amerykanskich studentów, korzysta dzis ponad 70% uzytkowników z innych panstw niz USA. W 2009 roku na portalu uruchomiono test nowych narzedzi, umozliwiajacych dostosowanie ochrony prywatnosci, jednakze tylko 20% uzytkowników rozpoczelo korzystanie z nich. Same serwisy wykorzystuja czesc powierzonych im danych uzytkowników do celów reklamowych. Osoba korzystajaca z Internetu ma mozliwosc usuniecia historii przegladanych stron, zablokowania niektórych ciasteczek (cookies) oraz wyskakujacych okienek, jednak nie zapewnia to pelnej ochrony prywatnosci.
Bezpieczenstwo[edytuj | edytuj kod]
Siec Web stala sie otwarta droga dla przestepców rozprzestrzeniajacych zlosliwe oprogramowanie. Cyberprzestepczosc prowadzona w internecie moze skladac sie z kradziezy tozsamosci, oszustw, szpiegostwa i gromadzenia poufnych informacji. Polaczenie z internetem przewyzsza tradycyjne zagrozenia dla bezpieczenstwa danych przetwarzanych przy pomocy komputera, a jak szacuje Google, okolo jedna na dziesiec stron internetowych moze zawierac zlosliwy kod. Wiekszosc ataków opartych na sieci Web odbywa sie z poziomu legalnych stron internetowych, a najczesciej, jak szacuje firma Sophos, producent oprogramowania antywirusowego, ataki sa prowadzone w Stanach Zjednoczonych, Chinach i Rosji. Najpowszechniejszym typem zagrozen jest SQL injection. Za pomoca jezyka HTML i URI siec Web zostala równiez narazona na ataki, takie jak cross-site scripting (XSS), które pojawily sie wraz z wprowadzeniem JavaScript, nastepnie zostaly rozszerzone do pewnego stopnia przez Web 2.0 i AJAX, uzywajace duzych ilosci skryptów. Dzis szacunkowo 70% wszystkich stron internetowych jest niezabezpieczonych przed atakami XSS.
Archiwizacja stron WWW[edytuj | edytuj kod]
Z biegiem czasu wiele zasobów publikowanych w Internecie zanika, zostaje przeniesionych, zaktualizowanych lub calkowicie zmienia sie ich zawartosc. To sprawia, ze niektóre odnosniki staja sie przestarzale. Okresla sie je wtedy mianem „martwych odnosników” (ang. dead links). Problem ten spowodowal, ze podjeto dzialania zaradcze, i np. Internet Archive, dzialajace od 1996 roku, jest obecnie najbardziej znana instytucja zajmujaca sie archiwizacja zasobów Internetu.
Standaryzacja[edytuj | edytuj kod]
Funkcjonowanie World Wide Web w Internecie oraz wymiana informacji pomiedzy komputerami opieraja sie na wielu standardach i specyfikacjach technicznych. Duza czesc tych dokumentów to opracowania World Wide Web Consortium (W3C), kierowanego przez Bernersa-Lee, a niektóre z nich sa dzielem Internet Engineering Task Force (IETF) oraz innych organizacji.
Gdy wspominamy o standardach internetowych, najczesciej mamy do czynienia z nastepujacymi publikacjami:
Zalecenia W3C dla jezyków znaczników, zwlaszcza HTML i XHTML. Okreslaja one struktury interpretacji dokumentów hipertekstowych.
Zalecenia W3C dla arkuszy stylów, szczególnie CSS.
Standardy ECMAScript (zazwyczaj w formie JavaScript), z ECMA International.
Zalecenia W3C dotyczace modelowania dokumentów obiektowych.
Dodatkowe publikacje dostarczaja definicji innych podstawowych technologii stosowanych w World Wide Web, m.in.:
Uniform Resource Identifier (URI), który jest uniwersalnym systemem odniesien do zasobów w Internecie, takich jak dokumenty hipertekstowe i obrazy. URI, czesto nazywane URL jest definiowane przez RFC 3986 ?.
Protokól HTTP, a konkretnie RFC 2616 ? i RFC 2617 ?, które okreslaja, jak przegladarka i serwer uwierzytelniaja siebie nawzajem.
Dostepnosc WWW[edytuj | edytuj kod]
Dostep do WWW jest mozliwy dla wszystkich, bez wzgledu na to, czy uzytkownik jest osoba w pelni czy niepelnosprawna. Niezaleznie od rodzaju niepelnosprawnosci, siec Web sluzy przesylaniu, jak równiez pozyskiwaniu informacji oraz interakcji ze spoleczenstwem, przez co niezmiernie wazne jest, aby umozliwic dostep do sieci osobom niepelnosprawnym, czesto ograniczonym ruchowo. Tim Berners-Lee twierdzi, iz „potega internetu polega na jego uniwersalnosci. Dostepnosc dla wszystkich, bez wzgledu na niepelnosprawnosc jest jej glównym aspektem”. Wiele krajów wymaga od autorów witryn stosowania ulatwien dostepu. Miedzynarodowa ustalenia w ramach „Inicjatywy dostepnosci do sieci” W3 Consortium (Web Accessibility Initiative) doprowadzily do wystosowania pewnych prostych wytycznych, które autorzy tresci internetowych, jak równiez twórcy oprogramowania moga wykorzystywac, aby umozliwic „surfowanie” po sieci osobom niepelnosprawnym.
Obsluga róznych jezyków[edytuj | edytuj kod]
W3C zapewnia, ze usluga WWW bedzie dostepna we wszystkich jezykach swiata. Na poczatku 2004 r. Unicode zyskal znaczna popularnosc i ostatecznie w grudniu 2007 r., jako najczesciej uzywany system kodowania znaków w sieci Web, wyparl zarówno ASCII, jak i zestaw znaków dla Europy Zachodniej. Pierwotnie zbiór RFC 3986 ? zezwalal obiektom sieci na identyfikacje tylko poprzez adresy skladajace sie ze znaków z podgrupy US-ASCII. RFC 3987 ? uznaje rozszerzony zakres znaków i od tej pory zasoby sieci moga byc identyfikowane poprzez adresowanie w kazdym z jezyków.
Predkosc transmisji[edytuj | edytuj kod]
Przeciazenie infrastruktury Internetu, wywolane przewaga popytu nad podaza, powoduje, nieraz znaczace, opóznienia w przegladaniu stron internetowych. Powstalo nawet zartobliwa okreslenie dla WWW: „World Wide Wait” (czyli: „wielkie, swiatowe czekanie”). Przyspieszenie dzialania sieci jest ciagle otwartym tematem dyskusji i rozwazan uzytkowników, specjalistów oraz polem dzialania technologii QoS. Inne rozwiazania majace na celu przyspieszenie Internetu mozna znalezc na stronie W3C[3].
Standardowe wytyczne dla idealnego czasu odpowiedzi z serwera to:
0,1 sekundy (jedna dziesiata sekundy) – idealny czas reakcji. Uzytkownik nie odczuwa jakichkolwiek opóznien.
1 sekunda – najwiekszy dopuszczalny czas reakcji. Czas odpowiedzi powyzej 1 sekundy zaklóca prace uzytkownika.
10 sekund – niedopuszczalny czas reakcji. Przegladanie zostanie przerwane, a uzytkownik prawdopodobnie ujrzy komunikat bledu.
Statystyki[edytuj | edytuj kod]
Wedlug badan z 2001 roku, istnialo wtedy wiecej niz 550 miliardów dokumentów internetowych, najczesciej dostepnych w tzw. „widocznym WWW” (ang. visible Web lub surface Web), czyli skatalogowanych (zindeksowanych) przez wyszukiwarki internetowe lub w tzw. „ukrytym WWW” (ang. hidden Web lub deep Web), czyli nieskatalogowanych przez wyszukiwarki. W 2002 r. przebadano zawartosc 2,024 miliarda stron WWW, dzieki czemu wiadomo, ze zdecydowanie najwiecej zawartosci sieci Web byla w jezyku angielskim: 56,4%; nastepne byly strony w jezyku niemieckim (7,7%), francuskim (5,6%) i japonskim (4,9%). Najnowsze badania wykorzystywaly do zebrania próbek stron internetowych wyszukiwarki internetowe w 75 róznych jezykach. Ustalono, ze od konca stycznia 2005 bylo ponad 11,5 miliarda publicznie indeksowanych stron internetowych. W marcu 2009 indeksowanych stron bylo juz co najmniej 25,21 miliarda. 25 lipca 2008 inzynierowie oprogramowania Google Jesse Alpert i Nissan Hajaj oglosili, iz wyszukiwarka odkryla bilion unikatowych adresów URL. Sposród 109,5 miliona domen dzialajacych w maju 2009 74% bylo komercyjne (tzn. strony dzialajace w domenie najwyzszego poziomu .com).